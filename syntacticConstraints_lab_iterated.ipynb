{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditions of Language Dynamics, week 5\n",
    "# Syntactic constraints lab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simulation features a replication of\n",
    "Culbertson & Smolensky (2012)'s model of learning biases for Greenberg’s Universal 18.\n",
    "\n",
    "|.       | N-Adj | Adj-N |\n",
    "|---------|-------|-------|\n",
    "|**Num-N**|  17%  | 27%   |\n",
    "|**N-Num**|  52%  | 4%    |\n",
    "\n",
    "The learning biases were previously experimentally explored in Culbertson, Smolensky & Legendre (2012). In this paper, the authors tested three hypotheses:\n",
    "\n",
    "1. Learning involves tracking input statistics\n",
    "2. Learners regularise variation\n",
    "3. Learners regularise but only orders that are easier to learn\n",
    "\n",
    "The model in Culbertson & Smolensky (2012) reformulates these hypotheses in terms of [Bayesian inference](https://en.wikipedia.org/wiki/Bayesian_inference) (which takes into account prior biases as well as the input likelihood) to find the best fit for the experimental data:\n",
    "\n",
    "1. Input likelihood given a flat/uninformative prior (no bias)\n",
    "2. Input likelihood given a regularisation prior (regularisation bias)\n",
    "3. Input likelihood given a regularisation prior and a harmonic prior (regularisation and harmony biases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The model uses Bayesian inference to predict the types of grammars learners will infer given (1) a set of counts of Adj-N, N-Adj, Num-N and N-Num utterances, and (2) their prior expectations in terms of variation (i.e., regularity) and ordering combinations (harmonicity). \n",
    "\n",
    "The first part of code imports all the libraries taht you will need to operate with the binomial and beta distributions, to do logs and exponentials, and to generate random numbers of various kinds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf')\n",
    "\n",
    "from scipy.stats import beta, binom\n",
    "from math import log, exp\n",
    "from scipy.special import logsumexp\n",
    "from numpy.random import choice, binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The input data\n",
    "\n",
    "The input consists of counts of [Adj-N, N-Adj, Num-N, N-Num] with 40 total per modifier type. Whether the input is skewed toward pre- or post-nominal modifiers for each phrase type depends on the condition, as shown in the table.\n",
    "\n",
    "\n",
    "|condition   | training counts|\n",
    "|------------|---------------|\n",
    "|1 | [28,12,28,12] |\n",
    "|2 | [12,28,12,28] |\n",
    "|3 | [12,28,28,12] |\n",
    "|4 | [28,12,12,28] |\n",
    "\n",
    "We’ll use a grid of probabilities to describe the space of possible generating grammars (i.e. hypotheses). We’ll need to use the grid twice–once for the probability of Adj-N (vs N-Adj), and again for the probability of Num-N (vs. N-Num)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data is a list of lists of counts for each condition:\n",
    "# (The list slots for each pattern correspond to [Adj-N, N-Adj, Num-N, N-Num]) \n",
    "#\n",
    "#                     pattern 1        pattern 2          pattern 3        pattern 4\n",
    "#                  (Adj-N,Num-N)     (N-Adj,N-Num)     (N-Adj,Num-N)     (Adj-N,N-Num)\n",
    "training_data = [[28, 12, 28, 12], [12, 28, 12, 28], [12, 28, 28, 12], [28, 12, 12, 28]]\n",
    "\n",
    "grid_granularity = 100 \n",
    "possible_p = []\n",
    "for i in range(1, grid_granularity):\n",
    "    possible_p.append(i / (grid_granularity + 0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check out `training_data`. How would you access just the counts for condition 2? Also take a look at the grid. What would a hypothesis of 0.99 mean?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood\n",
    "\n",
    "The likelihood here is calculated using the binomial distribution. We are interested in the likelihhod of the individual's inferred grammar (i.e., NP ordering rules) given the proportion of Adj-N in Adj trials, and the proportion of Num-N out in Num trials. \n",
    "\n",
    "***\n",
    "Wait a second? Binomial distribution?\n",
    "\n",
    "- Coin toss example: How many head out of total tosses? Is it a fair coin or a biased coin?\n",
    "\n",
    "        binomial(k = 5 heads | p = 0.5, n = 10 tosses) = 0.246\n",
    "        \n",
    "        binomial(k = 5 heads | p = 0.9, n = 10 tosses) = 0.001\n",
    "\n",
    "  Take a second to check the above [here](https://www.socscistatistics.com/tests/binomial/default2.aspx) and [here](https://shiny.rit.albany.edu/stat/binomial/).\n",
    "\n",
    "\n",
    "- Adj, N ordering example: How many Adj-N out of the total of Adj utterances? Does the grammar tend to use Adj-N?\n",
    "\n",
    "        binomial(k = 28 Adj-N | p = 0.5, n = 40 Adj) = 0.005 \n",
    "\n",
    "        binomial(k = 28 Adj-N | p = 0.7, n = 40 Adj) = 0.137\n",
    "***\n",
    "\n",
    "\n",
    "The function below calculates the log likelihood of the data, (where the data are counts representing number of Adj-N out of total Adj instances and number of Num-N out of total Num instances) given point probability of Adj-N, and Num-N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U18_likelihood(data, p_AdjN, p_NumN):\n",
    "    loglikelihood = []\n",
    "    loglikelihood_AdjN = binom.logpmf(data[0], data[0] + data[1], p_AdjN)\n",
    "    loglikelihood_NumN = binom.logpmf(data[2], data[2] + data[3], p_NumN)\n",
    "    loglikelihood = loglikelihood_AdjN + loglikelihood_NumN #summing logs = multiplying non-log\n",
    "    return loglikelihood   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check out U18_likelihood function. It returns log probabilities. Find the line of code in the function that would allow you to calculate the log likelihood of getting 28 Adj-N counts out of a total of 40 when the underlying probability of Adj-N according to the grammar is 0.7 (versus 0.3 of N-Adj). Calculate it! Why is the result different from the one above? How would you obtain the same result?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior\n",
    "\n",
    "The prior in this model has two parts. One part is regularisation as encoded by the parameters of the beta distribution. The two parameters of the beta distribution are (annoyingly) called alpha and beta. Here we want two separate asymmetrical beta distributions favoring either probabilities close to one or close to zero, but not both (i.e., we only expect one-way regularisation). \n",
    "\n",
    "The second part of the prior is harmony. This is the part pf the prior which favors particular combinations of orders (i.e., Adj-N with Num-N); in order to get it we need to promote or penalize particular parts of the two dimensional grammar space. \n",
    "\n",
    "We can do this by defining four “components\" using different combinations of alpha and beta. The graph below shows beta distributions with (alpha=10, beta=10), (alpha=15, beta=3), (alpha=15, beta=0.1) for Adj-N, and the reverse for Num-N:\n",
    "\n",
    "![alt text](https://www.morphdomains.uzh.ch/CarmenSaldana/CLD/labs/fig2.jpeg \"grammar space\")\n",
    "\n",
    "\n",
    "![alt text](https://www.morphdomains.uzh.ch/CarmenSaldana/CLD/labs/fig3.jpeg \"beta distributions\")\n",
    "\n",
    "Given these four components, we calculate the prior probability of any grammar. This will actually be the sum of its probability given the beta distributions governing p(Adj-N) and p(Num-N) for each component, weighted by the probability of that component. The latter is determined by gamma (g). If the probability of a given component (e.g., g[4], which corresponds to component 4) is low, then pattern 4 grammars very likely to be generated by it will have a low prior probability.\n",
    "\n",
    "The function below calculates the log prior probability of a given p_AdjN and p_NumN given a set of parameters. This is a sum over the probabilities given by each mixture component. Parameters are: g (set of four mixture weights), a, b (Beta shape parameters which can function as alpha or beta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U18_prior(g, a, b, p_AdjN, p_NumN):\n",
    "    pattern1_component = [a, b, a, b] # higher prob for Adj-N, Num-N\n",
    "    pattern2_component = [b, a, b, a] # higher prob for N-Adj, N-Num\n",
    "    pattern3_component = [b, a, a, b] # higher prob for N-Adj, Num-N\n",
    "    pattern4_component = [a, b, b, a] # higher prob for Adj-N, N-Num\n",
    "    components = [pattern1_component, pattern2_component, \n",
    "                  pattern3_component, pattern4_component]\n",
    "    logprior = []\n",
    "    for i in range(0,4): # loop over all four components\n",
    "        logprior_i_Adj = beta.logpdf(p_AdjN, components[i][0], components[i][1])\n",
    "        logprior_i_Num = beta.logpdf(p_NumN, components[i][2], components[i][3])\n",
    "        logprior_i = logprior_i_Adj + logprior_i_Num\n",
    "        logprior.append(logprior_i)\n",
    "    # a+b+... in log space = log(exp(a)+exp(b)+...)\n",
    "    logprior = log((g[0] * exp(logprior[0])) + \n",
    "                   (g[1] * exp(logprior[1])) + \n",
    "                   (g[2] * exp(logprior[2])) + \n",
    "                   (g[3] * exp(logprior[3])))\n",
    "    return logprior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take a look at the four components in the prior. Can you see why they are defined by those combinations of alpha and beta?*\n",
    "\n",
    "*Each component - a combination of two beta distributions - can in principle generate any grammar, that is any pair [p(Adj-N), p(Num-N)]. Which component assigns the highest probability to the pair [0.8,0.8]? How about [0.4,0.9]?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The optimal prior parameters\n",
    "\n",
    "The parameters of the prior are alpha, beta, gamma. The first two (called a, and b in the code) encode the regularization bias. The higher a is relative to b, the more regularization. The third, g is a set of weights for the four components, summing to 1. Culbertson & Smolensky (2012) searched through these parameters to find the ones that fit their Culbertson, Smolensky & Legendre (2012) experimental data (shown in the plot below) best.\n",
    "\n",
    "![alt text](https://www.morphdomains.uzh.ch/CarmenSaldana/CLD/labs/fig4.jpeg \"experimental data\")\n",
    "\n",
    "The best fit parameters were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                  gamma                         a      b\n",
    "fit_parameters = [[0.6293, 0.3706, 0.0001, 0], 16.5,  0.001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior probability of patterns given the fit parameters:\n",
    "\n",
    "![](https://www.morphdomains.uzh.ch/CarmenSaldana/CLD/labs/priors_CS12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior\n",
    "\n",
    "The posterior distribution over grammars can now be calculated using the likelihood and the prior with the fit parameters. This function calculates the log posterior probability of a set of counts for all possible p_AdjN, p_NumN combinations, given prior parameters g, a, b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U18_posterior(g, a, b, data):\n",
    "    posterior = [] \n",
    "    for p_a in range(len(possible_p)):\n",
    "        for p_n in range(len(possible_p)):\n",
    "            lik_i = U18_likelihood(data, possible_p[p_a], possible_p[p_n])\n",
    "            prior_i = U18_prior(g, a, b, possible_p[p_a], possible_p[p_n]) \n",
    "            posterior.append(lik_i + prior_i)  \n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Given the fit parameters, will the model predict a weak or strong regularization bias? Will a set of counts generated from a grammar like [0.8,0.2] (for [p(Adj-N), p(Num-N)])  have a high, medium or low posterior probability?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results\n",
    "\n",
    "The model can now be used to generate predicted learning outcomes; given some input data, and the prior parameters, we can generate sampled grammars and plot them in the two-dimensional grammar space we’ve be using. The figure belows shows this as reported in the original Culbertson & Smolensky (2012) paper. \n",
    "\n",
    "![alt text](https://www.morphdomains.uzh.ch/CarmenSaldana/CLD/labs/fig5.jpeg \"culbertson & smolensky results\")\n",
    "\n",
    "You can use the `U18_roulette_wheel` function to reproduce this. This function samples pairs of p(Adj-N), p(Num-N) probabilities, and it lets you specify how many such samples you want. You’ll notice there’s also a normalisation function `normalize_log_distribution` which takes log probabilities output by the posterior and makes them a proper (non-log) probability distribution for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_log_distribution(distribution):\n",
    "    exp_dist = []\n",
    "    for logp in distribution:\n",
    "        exp_dist.append(exp(logp))\n",
    "    norm_dist = []\n",
    "    for p in exp_dist:\n",
    "        norm_dist.append(p / sum(exp_dist))\n",
    "    return norm_dist\n",
    "\n",
    "\n",
    "def U18_roulette_wheel(g, a, b, data, num_samps):\n",
    "    post = U18_posterior(g, a, b, data) # calculate posterior given training data and prior parameters\n",
    "    post = normalize_log_distribution(post)\n",
    "    # make a grid of all possible p_AdjN, p_NumN combinations at the granularity specified\n",
    "    grid = []\n",
    "    for p_a in range(len(possible_p)):\n",
    "        for p_n in range(len(possible_p)):\n",
    "            grid.append([possible_p[p_a], possible_p[p_n]])\n",
    "    # samples some grammars!\n",
    "    grammars = []\n",
    "    for i in range(num_samps):\n",
    "        r = choice(a=range(len(grid)), p=post)   # choose an index from the grid according to it's posterior probability  \n",
    "        grammars.append(grid[r])\n",
    "    return grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a function to plot the results as a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grammars(g_1, g_2, g_3, g_4):\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for g in g_1:\n",
    "        plt.scatter([g[0]], [g[1]], color='red', alpha=0.5)\n",
    "\n",
    "    for g in g_2:\n",
    "        plt.scatter([g[0]], [g[1]], color='blue', alpha=0.5)\n",
    "\n",
    "    for g in g_3:\n",
    "        plt.scatter([g[0]], [g[1]], color='orange', alpha=0.5)\n",
    "\n",
    "    for g in g_4:\n",
    "        plt.scatter([g[0]], [g[1]], color='purple', alpha=0.5)\n",
    "\n",
    "    plt.xlabel(\"P(Adj-N)\")\n",
    "    plt.ylabel(\"P(Num-N)\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. Generate samples `g_1, g_2, g_3, g_4` using function calls like:\n",
    "```python\n",
    "g_1 = U18_roulette_wheel(fit_parameters[0], fit_parameters[1], fit_parameters[2], training_data[0], 100)\n",
    "```\n",
    "Then call the plotting function to plot them. Does your plot look like figure above from the paper? (N.B. This might take a while to run.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_1 =  U18_roulette_wheel(fit_parameters[0], fit_parameters[1], fit_parameters[2], training_data[0], 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What does it mean that the highest value of g is for component 1? Why do you think that might be the highest given the population of learners tested in the experiment? Come up with a new set of g values that you think would more accurately reflect the typology (e.g., in table at the start of the notebook) and redo the samples and plot. Did it turn out as you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What do you think would happen if the regularization bias were not as strong? Change the a,b parameters and see if you were right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking it further: iterating the model \n",
    "\n",
    "The remaining functions in this notebook are for iterating the model - taking the original training data, generating posterior probability distributions for all four conditions, sampling some grammars from each posterior and counting the number of each pattern type that results. Then, generating some new training data from one of the sampled grammars from each condition to pass on to the next generation. And so on.\n",
    "\n",
    "The first function classifies a given grammar as one of the four patterns. So if a learner acquires (0.7,0.8) that would be classified as a pattern 1 grammar. We’ll need this to track the counts of each pattern type over generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U18_classify(p_AdjN, p_NumN):\n",
    "    if p_AdjN > 0.5 and p_NumN > 0.5: return 1\n",
    "    if p_AdjN < 0.5 and p_NumN < 0.5: return 2 \n",
    "    if p_AdjN < 0.5 and p_NumN > 0.5: return 3\n",
    "    if p_AdjN > 0.5 and p_NumN < 0.5: return 4\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function generates training data given a grammar by taking sampled counts from a binomial distribution. We’ll need this to create new training data to pass on to the next generation of learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U18_produce(p_AdjN, p_NumN):\n",
    "    AdjN = binomial(n=40, p=p_AdjN) # number of Adj-N out of n trials with p=p_AdjN\n",
    "    NumN = binomial(n=40, p=p_NumN) # number of Num-N out of n trials with p=p_NumN\n",
    "    return [AdjN, 40-AdjN, NumN, 40-NumN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, a pretty horrendous looking function does the iterating. This iterates from starting data consisting of counts of Adj-N, Num-N. The steps it follows are: \n",
    "\n",
    "1. get samples from each condition given starting_data\n",
    "2. pick random grammar from each set of samples and use to generate new starting_data for that condition\n",
    "3. count the number of each patterns resulting from those samples\n",
    "4. REPEAT\n",
    "\n",
    "This function returns a list of lists. Each list in there tracks the counts for one of the four patterns over generations. If you ran 3 generations there’d be three numbers in each list representing how many of each pattern resulted from learning in that generation.\n",
    "\n",
    "There are four loops. The outermost loop goes through the generations (as many as the user specifies). The second loop goes through each condition for the current generation, samples some grammars from the posterior for that condition and picks one of them to generate new training data from, to pass on to the next generation. The third loop classifies all the sampled patterns. The fourth loop puts the counts of each pattern for the current generation into the value of the function to be output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U18_iterate(starting_data, g, a, b, generations, num_samps):\n",
    "    \n",
    "    pattern_tracer=[[], [], [], []] # value of the function, each sublist \n",
    "                                    # tracks count of each pattern type over generations   \n",
    "    for gen in range(generations):\n",
    "        patterns_g = [] # accumulator for pattern types in each sample for current generation\n",
    "        new_data=[[], [], [], []] # to be used as starting_data in the subsequent generation\n",
    "        \n",
    "        # for each condition, get sample of grammars, generate new training data, \n",
    "        # count patterns...\n",
    "        for i in range(4):\n",
    "            print(\"sampling for condition \" + str(i+1) + \"...\")\n",
    "            samps_i = U18_roulette_wheel(g, a, b, starting_data[i], num_samps) # get sample of grammars\n",
    "            r = random.randint(0, num_samps - 1)# pick a random index from samps\n",
    "            training_g = samps_i[r] # get the grammar at index r \n",
    "            print(\"learner has acquired: \" + str(training_g))\n",
    "            new_data[i] = U18_produce(training_g[0], training_g[1]) # use grammar to generate new data\n",
    "            print(\"passing on:\" + str(new_data[i]) + \" to next generation...\")\n",
    "            # now for each grammar in the sample, classify it and add pattern to the accumulator\n",
    "            for s in range(len(samps_i)):\n",
    "                patterns_g.append(U18_classify(samps_i[s][0], samps_i[s][1]))\n",
    "        \n",
    "        # go through pattern accumulator and count each type for the current generation\n",
    "        for i in range(4): \n",
    "            pattern_tracer[i].append(patterns_g.count(i + 1)) # add the set of patterns to list\n",
    "        print(\"Frequency of patterns for this generation: \" + str(pattern_tracer))\n",
    "            \n",
    "        starting_data = new_data # make the new training data the starting data\n",
    "    \n",
    "    return pattern_tracer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do some iterating, call the function with the following arguments. It will take a while, and you’ll notice that the function has some print statements so you know what it’s up to while you’re waiting.\n",
    "\n",
    "```python\n",
    "counts = U18_iterate(training_data, fit_parameters[0], fit_parameters[1], fit_parameters[2], 3, 100)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot the outcome of the iterated learning simulation, you can use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_counts(counts,generations):\n",
    "    '''\n",
    "    Plot count of grammars over generations (output of U18_iterate() call)\n",
    "    '''\n",
    "    m = (max(counts[0]),max(counts[1]),max(counts[2]),max(counts[3]))\n",
    "    lim = max(m)+50\n",
    "    colors=['red','blue','orange','purple']\n",
    "    \n",
    "    plt.title(\"Counts of pattern type over generations\")\n",
    "    plt.xlim(1,generations);plt.ylim(0,lim)\n",
    "    plt.xlabel(\"Generation\");plt.ylabel(\"Count\")\n",
    "    plt.xticks(range(1,generations+1))\n",
    "   \n",
    "    for i in range(0,4):\n",
    "        plt.plot(range(1,generations+1),counts[i],color=colors[i],label='pattern '+str(i+1))\n",
    "    plt.legend(loc='upper left',fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will show you which patterns survive, and which die out over generations. Do you think this is a realistic result? Why or why not?\n",
    "\n",
    "You can play around and change fit_parameters when running U18_iterate( ) to see whether or not your predictions about these changes match the outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
